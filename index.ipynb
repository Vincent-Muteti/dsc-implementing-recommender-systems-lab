{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Recommender Systems - Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, you'll practice creating a recommender system model using `surprise`. You'll also get the chance to create a more complete recommender system pipeline to obtain the top recommendations for a specific user.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "In this lab you will: \n",
    "\n",
    "- Use surprise's built-in reader class to process data to work with recommender algorithms \n",
    "- Obtain a prediction for a specific user for a particular item \n",
    "- Introduce a new user with rating to a rating matrix and make recommendations for them \n",
    "- Create a function that will return the top n recommendations for a user \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, we will be using the famous 1M movie dataset. It contains a collection of user ratings for many different movies. In the last lesson, you were exposed to working with `surprise` datasets. In this lab, you will also go through the process of reading in a dataset into the `surprise` dataset format. To begin with, load the dataset into a Pandas DataFrame. Determine which columns are necessary for your recommendation system and drop any extraneous ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100836 entries, 0 to 100835\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   userId     100836 non-null  int64  \n",
      " 1   movieId    100836 non-null  int64  \n",
      " 2   rating     100836 non-null  float64\n",
      " 3   timestamp  100836 non-null  int64  \n",
      "dtypes: float64(1), int64(3)\n",
      "memory usage: 3.1 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./ml-latest-small/ratings.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "new_df = df[['userId', 'movieId', 'rating']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now time to transform the dataset into something compatible with `surprise`. In order to do this, you're going to need `Reader` and `Dataset` classes. There's a method in `Dataset` specifically for loading dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  √ó Building wheel for scikit-surprise (pyproject.toml) did not run successfully.\n",
      "  ‚îÇ exit code: 1\n",
      "  ‚ï∞‚îÄ> [155 lines of output]\n",
      "      C:\\Users\\Moringa School\\AppData\\Local\\Temp\\pip-build-env-ju79xcp1\\overlay\\Lib\\site-packages\\setuptools\\config\\_apply_pyprojecttoml.py:82: SetuptoolsDeprecationWarning: `project.license` as a TOML table is deprecated\n",
      "      !!\n",
      "      \n",
      "              ********************************************************************************\n",
      "              Please use a simple string containing a SPDX expression for `project.license`. You can also use `project.license-files`. (Both options available on setuptools>=77.0.0).\n",
      "      \n",
      "              By 2026-Feb-18, you need to update your project and remove deprecated calls\n",
      "              or your builds will no longer be supported.\n",
      "      \n",
      "              See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
      "              ********************************************************************************\n",
      "      \n",
      "      !!\n",
      "        corresp(dist, value, root_dir)\n",
      "      C:\\Users\\Moringa School\\AppData\\Local\\Temp\\pip-build-env-ju79xcp1\\overlay\\Lib\\site-packages\\setuptools\\config\\_apply_pyprojecttoml.py:61: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
      "      !!\n",
      "      \n",
      "              ********************************************************************************\n",
      "              Please consider removing the following classifiers in favor of a SPDX license expression:\n",
      "      \n",
      "              License :: OSI Approved :: BSD License\n",
      "      \n",
      "              See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
      "              ********************************************************************************\n",
      "      \n",
      "      !!\n",
      "        dist._finalize_license_expression()\n",
      "      C:\\Users\\Moringa School\\AppData\\Local\\Temp\\pip-build-env-ju79xcp1\\overlay\\Lib\\site-packages\\setuptools\\dist.py:759: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
      "      !!\n",
      "      \n",
      "              ********************************************************************************\n",
      "              Please consider removing the following classifiers in favor of a SPDX license expression:\n",
      "      \n",
      "              License :: OSI Approved :: BSD License\n",
      "      \n",
      "              See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
      "              ********************************************************************************\n",
      "      \n",
      "      !!\n",
      "        self._finalize_license_expression()\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\\lib.win-amd64-cpython-311\\surprise\n",
      "      copying surprise\\accuracy.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "      copying surprise\\builtin_datasets.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "      copying surprise\\dataset.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "      copying surprise\\dump.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "      copying surprise\\reader.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "      copying surprise\\trainset.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "      copying surprise\\utils.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "      copying surprise\\__init__.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "      copying surprise\\__main__.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "      creating build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
      "      copying surprise\\model_selection\\search.py -> build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
      "      copying surprise\\model_selection\\split.py -> build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
      "      copying surprise\\model_selection\\validation.py -> build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
      "      copying surprise\\model_selection\\__init__.py -> build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
      "      creating build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "      copying surprise\\prediction_algorithms\\algo_base.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "      copying surprise\\prediction_algorithms\\baseline_only.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "      copying surprise\\prediction_algorithms\\knns.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "      copying surprise\\prediction_algorithms\\predictions.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "      copying surprise\\prediction_algorithms\\random_pred.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "      copying surprise\\prediction_algorithms\\__init__.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "      running egg_info\n",
      "      writing scikit_surprise.egg-info\\PKG-INFO\n",
      "      writing dependency_links to scikit_surprise.egg-info\\dependency_links.txt\n",
      "      writing entry points to scikit_surprise.egg-info\\entry_points.txt\n",
      "      writing requirements to scikit_surprise.egg-info\\requires.txt\n",
      "      writing top-level names to scikit_surprise.egg-info\\top_level.txt\n",
      "      dependency C:\\Users\\Moringa School\\AppData\\Local\\Temp\\pip-build-env-ju79xcp1\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayobject.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\Moringa School\\AppData\\Local\\Temp\\pip-build-env-ju79xcp1\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayscalars.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\Moringa School\\AppData\\Local\\Temp\\pip-build-env-ju79xcp1\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarrayobject.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\Moringa School\\AppData\\Local\\Temp\\pip-build-env-ju79xcp1\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarraytypes.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\Moringa School\\AppData\\Local\\Temp\\pip-build-env-ju79xcp1\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ufuncobject.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\Moringa School\\AppData\\Local\\Temp\\pip-build-env-ju79xcp1\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayobject.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\Moringa School\\AppData\\Local\\Temp\\pip-build-env-ju79xcp1\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayscalars.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\Moringa School\\AppData\\Local\\Temp\\pip-build-env-ju79xcp1\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarrayobject.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\Moringa School\\AppData\\Local\\Temp\\pip-build-env-ju79xcp1\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarraytypes.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\Moringa School\\AppData\\Local\\Temp\\pip-build-env-ju79xcp1\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ufuncobject.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\Moringa School\\AppData\\Local\\Temp\\pip-build-env-ju79xcp1\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayobject.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\Moringa School\\AppData\\Local\\Temp\\pip-build-env-ju79xcp1\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayscalars.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\Moringa School\\AppData\\Local\\Temp\\pip-build-env-ju79xcp1\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarrayobject.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\Moringa School\\AppData\\Local\\Temp\\pip-build-env-ju79xcp1\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarraytypes.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\Moringa School\\AppData\\Local\\Temp\\pip-build-env-ju79xcp1\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ufuncobject.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\Moringa School\\AppData\\Local\\Temp\\pip-build-env-ju79xcp1\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayobject.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\Moringa School\\AppData\\Local\\Temp\\pip-build-env-ju79xcp1\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayscalars.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\Moringa School\\AppData\\Local\\Temp\\pip-build-env-ju79xcp1\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarrayobject.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\Moringa School\\AppData\\Local\\Temp\\pip-build-env-ju79xcp1\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarraytypes.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\Moringa School\\AppData\\Local\\Temp\\pip-build-env-ju79xcp1\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ufuncobject.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\Moringa School\\AppData\\Local\\Temp\\pip-build-env-ju79xcp1\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayobject.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\Moringa School\\AppData\\Local\\Temp\\pip-build-env-ju79xcp1\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayscalars.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\Moringa School\\AppData\\Local\\Temp\\pip-build-env-ju79xcp1\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarrayobject.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\Moringa School\\AppData\\Local\\Temp\\pip-build-env-ju79xcp1\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarraytypes.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\Moringa School\\AppData\\Local\\Temp\\pip-build-env-ju79xcp1\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ufuncobject.h won't be automatically included in the manifest: the path must be relative\n",
      "      reading manifest file 'scikit_surprise.egg-info\\SOURCES.txt'\n",
      "      reading manifest template 'MANIFEST.in'\n",
      "      warning: no previously-included files matching '*.so' found under directory 'surprise'\n",
      "      adding license file 'LICENSE.md'\n",
      "      writing manifest file 'scikit_surprise.egg-info\\SOURCES.txt'\n",
      "      C:\\Users\\Moringa School\\AppData\\Local\\Temp\\pip-build-env-ju79xcp1\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:212: _Warning: Package 'surprise.prediction_algorithms' is absent from the `packages` configuration.\n",
      "      !!\n",
      "      \n",
      "              ********************************************************************************\n",
      "              ############################\n",
      "              # Package would be ignored #\n",
      "              ############################\n",
      "              Python recognizes 'surprise.prediction_algorithms' as an importable package[^1],\n",
      "              but it is absent from setuptools' `packages` configuration.\n",
      "      \n",
      "              This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "              package, please make sure that 'surprise.prediction_algorithms' is explicitly added\n",
      "              to the `packages` configuration field.\n",
      "      \n",
      "              Alternatively, you can also rely on setuptools' discovery methods\n",
      "              (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "              instead of `find_packages(...)`/`find:`).\n",
      "      \n",
      "              You can read more about \"package discovery\" on setuptools documentation page:\n",
      "      \n",
      "              - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "      \n",
      "              If you don't want 'surprise.prediction_algorithms' to be distributed and are\n",
      "              already explicitly excluding 'surprise.prediction_algorithms' via\n",
      "              `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "              you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "              combination with a more fine grained `package-data` configuration.\n",
      "      \n",
      "              You can read more about \"package data files\" on setuptools documentation page:\n",
      "      \n",
      "              - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "      \n",
      "      \n",
      "              [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                    even if it does not contain any `.py` files.\n",
      "                    On the other hand, currently there is no concept of package data\n",
      "                    directory, all directories are treated like packages.\n",
      "              ********************************************************************************\n",
      "      \n",
      "      !!\n",
      "        check.warn(importable)\n",
      "      copying surprise\\similarities.c -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "      copying surprise\\similarities.pyx -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "      copying surprise\\prediction_algorithms\\co_clustering.c -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "      copying surprise\\prediction_algorithms\\matrix_factorization.c -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "      copying surprise\\prediction_algorithms\\optimize_baselines.c -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "      copying surprise\\prediction_algorithms\\slope_one.c -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "      copying surprise\\prediction_algorithms\\co_clustering.pyx -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "      copying surprise\\prediction_algorithms\\matrix_factorization.pyx -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "      copying surprise\\prediction_algorithms\\optimize_baselines.pyx -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "      copying surprise\\prediction_algorithms\\slope_one.pyx -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "      running build_ext\n",
      "      building 'surprise.similarities' extension\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for scikit-surprise\n",
      "ERROR: Could not build wheels for scikit-surprise, which is required to install pyproject.toml-based projects\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-surprise\n",
      "  Using cached scikit_surprise-1.1.4.tar.gz (154 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: still running...\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\moringa school\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\moringa school\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\moringa school\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.11.4)\n",
      "Building wheels for collected packages: scikit-surprise\n",
      "  Building wheel for scikit-surprise (pyproject.toml): started\n",
      "  Building wheel for scikit-surprise (pyproject.toml): finished with status 'error'\n",
      "Failed to build scikit-surprise\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-surprise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-surprise\n",
      "  Using cached scikit_surprise-1.1.4.tar.gz (154 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  √ó Getting requirements to build wheel did not run successfully.\n",
      "  ‚îÇ exit code: 1\n",
      "  ‚ï∞‚îÄ> [44 lines of output]\n",
      "      Compiling surprise/similarities.pyx because it changed.\n",
      "      Compiling surprise/prediction_algorithms/matrix_factorization.pyx because it changed.\n",
      "      Compiling surprise/prediction_algorithms/optimize_baselines.pyx because it changed.\n",
      "      Compiling surprise/prediction_algorithms/slope_one.pyx because it changed.\n",
      "      Compiling surprise/prediction_algorithms/co_clustering.pyx because it changed.\n",
      "      [1/5] Cythonizing surprise/prediction_algorithms/co_clustering.pyx\n",
      "      \n",
      "      Error compiling Cython file:\n",
      "      ------------------------------------------------------------\n",
      "      ...\n",
      "              self.avg_cltr_i = avg_cltr_i\n",
      "              self.avg_cocltr = avg_cocltr\n",
      "      \n",
      "              return self\n",
      "      \n",
      "          def compute_averages(self, np.ndarray[np.int_t] cltr_u,\n",
      "                                                   ^\n",
      "      ------------------------------------------------------------\n",
      "      surprise\\prediction_algorithms\\co_clustering.pyx:157:45: Invalid type.\n",
      "      Traceback (most recent call last):\n",
      "        File \u001b[35m\"c:\\Users\\Moringa School\\anaconda3\\envs\\learn-env\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\"\u001b[0m, line \u001b[35m389\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "          \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "          \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "        File \u001b[35m\"c:\\Users\\Moringa School\\anaconda3\\envs\\learn-env\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\"\u001b[0m, line \u001b[35m373\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "          json_out[\"return_val\"] = \u001b[31mhook\u001b[0m\u001b[1;31m(**hook_input[\"kwargs\"])\u001b[0m\n",
      "                                   \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "        File \u001b[35m\"c:\\Users\\Moringa School\\anaconda3\\envs\\learn-env\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\"\u001b[0m, line \u001b[35m143\u001b[0m, in \u001b[35mget_requires_for_build_wheel\u001b[0m\n",
      "          return hook(config_settings)\n",
      "        File \u001b[35m\"C:\\Users\\Moringa School\\AppData\\Local\\Temp\\pip-build-env-ilytvzw1\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m331\u001b[0m, in \u001b[35mget_requires_for_build_wheel\u001b[0m\n",
      "          return \u001b[31mself._get_build_requires\u001b[0m\u001b[1;31m(config_settings, requirements=[])\u001b[0m\n",
      "                 \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\Moringa School\\AppData\\Local\\Temp\\pip-build-env-ilytvzw1\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m301\u001b[0m, in \u001b[35m_get_build_requires\u001b[0m\n",
      "          \u001b[31mself.run_setup\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "          \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\Moringa School\\AppData\\Local\\Temp\\pip-build-env-ilytvzw1\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m317\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
      "          \u001b[31mexec\u001b[0m\u001b[1;31m(code, locals())\u001b[0m\n",
      "          \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^\u001b[0m\n",
      "        File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m116\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\Moringa School\\AppData\\Local\\Temp\\pip-build-env-ilytvzw1\\overlay\\Lib\\site-packages\\Cython\\Build\\Dependencies.py\"\u001b[0m, line \u001b[35m1153\u001b[0m, in \u001b[35mcythonize\u001b[0m\n",
      "          \u001b[31mcythonize_one\u001b[0m\u001b[1;31m(*args)\u001b[0m\n",
      "          \u001b[31m~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\Moringa School\\AppData\\Local\\Temp\\pip-build-env-ilytvzw1\\overlay\\Lib\\site-packages\\Cython\\Build\\Dependencies.py\"\u001b[0m, line \u001b[35m1297\u001b[0m, in \u001b[35mcythonize_one\u001b[0m\n",
      "          raise CompileError(None, pyx_file)\n",
      "      \u001b[1;35mCython.Compiler.Errors.CompileError\u001b[0m: \u001b[35msurprise/prediction_algorithms/co_clustering.pyx\u001b[0m\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "√ó Getting requirements to build wheel did not run successfully.\n",
      "‚îÇ exit code: 1\n",
      "‚ï∞‚îÄ> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!\"{sys.executable}\" -m pip install scikit-surprise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surprise imported successfully üéâ\n"
     ]
    }
   ],
   "source": [
    "from surprise import Reader, Dataset\n",
    "print(\"Surprise imported successfully üéâ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Moringa School\\anaconda3\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Reader, Dataset\n",
    "# read in values as Surprise dataset \n",
    "# Define the rating scale\n",
    "reader = Reader(rating_scale=(0.5, 5.0))\n",
    "\n",
    "# Load the DataFrame into a Surprise Dataset\n",
    "data = Dataset.load_from_df(new_df[['userId', 'movieId', 'rating']], reader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how many users and items we have in our dataset. If using neighborhood-based methods, this will help us determine whether or not we should perform user-user or item-item similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users:  610 \n",
      "\n",
      "Number of items:  9724\n"
     ]
    }
   ],
   "source": [
    "dataset = data.build_full_trainset()\n",
    "print('Number of users: ', dataset.n_users, '\\n')\n",
    "print('Number of items: ', dataset.n_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine the best model \n",
    "\n",
    "Now, compare the different models and see which ones perform best. For consistency sake, use RMSE to evaluate models. Remember to cross-validate! Can you get a model with a higher average RMSE on test data than 0.869?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing relevant libraries\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise.prediction_algorithms import SVD\n",
    "from surprise.prediction_algorithms import KNNWithMeans, KNNBasic, KNNBaseline\n",
    "from surprise.model_selection import GridSearchCV\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   19.9s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:  5.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RMSE: 0.8550552663141554\n",
      "Best Params: {'n_factors': 150, 'n_epochs': 40, 'lr_all': 0.007, 'reg_all': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 405 out of 405 | elapsed:  6.9min finished\n"
     ]
    }
   ],
   "source": [
    "## Perform a gridsearch with SVD\n",
    "# ‚è∞ This cell may take several minutes to run\n",
    "# Define the parameter grid for SVD\n",
    "param_grid = {\n",
    "    'n_factors': [50, 100, 150],\n",
    "    'n_epochs': [20, 30, 40],\n",
    "    'lr_all': [0.002, 0.005, 0.007],\n",
    "    'reg_all': [0.02, 0.05, 0.1]\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "gs = GridSearchCV(\n",
    "    SVD,\n",
    "    param_grid,\n",
    "    measures=['rmse'],\n",
    "    cv=5,          # 5-fold cross validation\n",
    "    joblib_verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "gs.fit(data)\n",
    "\n",
    "# Print best RMSE and parameters\n",
    "print(\"Best RMSE:\", gs.best_score['rmse'])\n",
    "print(\"Best Params:\", gs.best_params['rmse'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RMSE score: 0.8550552663141554\n",
      "Best parameters for SVD: {'n_factors': 150, 'n_epochs': 40, 'lr_all': 0.007, 'reg_all': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# print out optimal parameters for SVD after GridSearch\n",
    "# Print best RMSE score\n",
    "print(\"Best RMSE score:\", gs.best_score['rmse'])\n",
    "\n",
    "# Print best parameters\n",
    "print(\"Best parameters for SVD:\", gs.best_params['rmse'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating RMSE of algorithm KNNBasic on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.9467  0.9434  0.9442  0.9413  0.9538  0.9459  0.0043  \n",
      "Fit time          0.40    0.53    0.51    0.44    0.40    0.46    0.05    \n",
      "Test time         3.19    3.62    3.19    2.49    2.29    2.96    0.49    \n",
      "Average RMSE: 0.945853474790642\n"
     ]
    }
   ],
   "source": [
    "# cross validating with KNNBasic\n",
    "# Define the KNNBasic algorithm\n",
    "knn = KNNBasic()\n",
    "\n",
    "# Cross-validate using RMSE\n",
    "results = cross_validate(knn, data, measures=['rmse'], cv=5, verbose=True)\n",
    "\n",
    "# Print the average RMSE\n",
    "print(\"Average RMSE:\", np.mean(results['test_rmse']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating RMSE of algorithm KNNBasic on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.9456  0.9532  0.9552  0.9474  0.9457  0.9494  0.0040  \n",
      "Fit time          0.43    0.67    0.49    0.30    0.58    0.50    0.13    \n",
      "Test time         3.39    2.83    2.76    1.80    2.79    2.71    0.51    \n",
      "Average RMSE (Test Set): 0.9494257140724554\n"
     ]
    }
   ],
   "source": [
    "# print out the average RMSE score for the test set\n",
    "# Define the KNNBasic algorithm\n",
    "knn = KNNBasic()\n",
    "\n",
    "# Perform 5-fold cross validation\n",
    "results = cross_validate(knn, data, measures=['rmse'], cv=5, verbose=True)\n",
    "\n",
    "# Calculate average RMSE on test set\n",
    "avg_rmse = np.mean(results['test_rmse'])\n",
    "\n",
    "print(\"Average RMSE (Test Set):\", avg_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating RMSE of algorithm KNNBaseline on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.8743  0.8777  0.8721  0.8743  0.8788  0.8754  0.0024  \n",
      "Fit time          1.20    0.85    1.00    0.94    1.78    1.15    0.33    \n",
      "Test time         6.67    2.28    4.58    4.25    3.17    4.19    1.48    \n",
      "Average RMSE (Test Set): 0.8754400754264673\n"
     ]
    }
   ],
   "source": [
    "# cross validating with KNNBaseline\n",
    "# Define the KNNBaseline algorithm\n",
    "knn_baseline = KNNBaseline()\n",
    "\n",
    "# Perform 5-fold cross validation\n",
    "results = cross_validate(knn_baseline, data, measures=['rmse'], cv=5, verbose=True)\n",
    "\n",
    "# Calculate average RMSE on test set\n",
    "avg_rmse = np.mean(results['test_rmse'])\n",
    "\n",
    "print(\"Average RMSE (Test Set):\", avg_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating RMSE of algorithm KNNBaseline on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.8781  0.8708  0.8747  0.8777  0.8742  0.8751  0.0026  \n",
      "Fit time          0.93    0.69    0.89    0.75    0.88    0.83    0.09    \n",
      "Test time         2.68    3.28    2.75    2.72    2.71    2.83    0.23    \n",
      "Average RMSE (Test Set): 0.8751107449021346\n"
     ]
    }
   ],
   "source": [
    "# print out the average score for the test set\n",
    "# Define the KNNBaseline algorithm\n",
    "knn_baseline = KNNBaseline()\n",
    "\n",
    "# Perform 5-fold cross validation\n",
    "results = cross_validate(knn_baseline, data, measures=['rmse'], cv=5, verbose=True)\n",
    "\n",
    "# Calculate average RMSE on test set\n",
    "avg_rmse = np.mean(results['test_rmse'])\n",
    "\n",
    "print(\"Average RMSE (Test Set):\", avg_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based off these outputs, it seems like the best performing model is the SVD model with `n_factors = 50` and a regularization rate of 0.05. Use that model or if you found one that performs better, feel free to use that to make some predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Recommendations\n",
    "\n",
    "It's important that the output for the recommendation is interpretable to people. Rather than returning the `movie_id` values, it would be far more valuable to return the actual title of the movie. As a first step, let's read in the movies to a dataframe and take a peek at what information we have about them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies = pd.read_csv('./ml-latest-small/movies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId                               title  \\\n",
       "0        1                    Toy Story (1995)   \n",
       "1        2                      Jumanji (1995)   \n",
       "2        3             Grumpier Old Men (1995)   \n",
       "3        4            Waiting to Exhale (1995)   \n",
       "4        5  Father of the Bride Part II (1995)   \n",
       "\n",
       "                                        genres  \n",
       "0  Adventure|Animation|Children|Comedy|Fantasy  \n",
       "1                   Adventure|Children|Fantasy  \n",
       "2                               Comedy|Romance  \n",
       "3                         Comedy|Drama|Romance  \n",
       "4                                       Comedy  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making simple predictions\n",
    "Just as a reminder, let's look at how you make a prediction for an individual user and item. First, we'll fit the SVD model we had from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x2b57d6c6ed0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd = SVD(n_factors= 50, reg_all=0.05)\n",
    "svd.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(uid=2, iid=4, r_ui=None, est=3.106333630907246, details={'was_impossible': False})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd.predict(2, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This prediction value is a tuple and each of the values within it can be accessed by way of indexing. Now let's put our knowledge of recommendation systems to do something interesting: making predictions for a new user!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining User Ratings \n",
    "\n",
    "It's great that we have working models and everything, but wouldn't it be nice to get to recommendations specifically tailored to your preferences? That's what we'll be doing now. The first step is to create a function that allows us to pick randomly selected movies. The function should present users with a movie and ask them to rate it. If they have not seen the movie, they should be able to skip rating it. \n",
    "\n",
    "The function `movie_rater()` should take as parameters: \n",
    "\n",
    "* `movie_df`: DataFrame - a dataframe containing the movie ids, name of movie, and genres\n",
    "* `num`: int - number of ratings\n",
    "* `genre`: string - a specific genre from which to draw movies\n",
    "\n",
    "The function returns:\n",
    "* rating_list : list - a collection of dictionaries in the format of {'userId': int , 'movieId': int , 'rating': float}\n",
    "\n",
    "#### This function is optional, but fun :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movie_rater(movie_df,num, genre=None):\n",
    "    pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# try out the new function here!\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "def movie_rater(movie_df, num, genre=None):\n",
    "    \"\"\"\n",
    "    Asks the user to rate a number of movies.\n",
    "    Returns a list of dictionaries: {'userId': int, 'movieId': int, 'rating': float}\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter by genre if provided\n",
    "    if genre:\n",
    "        movie_df = movie_df[movie_df['genres'].str.contains(genre, regex=False)]\n",
    "\n",
    "    rating_list = []\n",
    "\n",
    "    # Randomly pick movies\n",
    "    sampled_movies = movie_df.sample(num)\n",
    "\n",
    "    # Create a new user ID (just use a high number to avoid conflicts)\n",
    "    new_user_id = movie_df['movieId'].max() + 1\n",
    "\n",
    "    for _, row in sampled_movies.iterrows():\n",
    "        print(f\"\\nMovie: {row['title']} | Genres: {row['genres']}\")\n",
    "        rating = input(\"Rate this movie from 1-5 (or press enter to skip): \")\n",
    "\n",
    "        if rating.strip() == \"\":\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            rating = float(rating)\n",
    "            if rating < 1 or rating > 5:\n",
    "                print(\"Rating must be between 1 and 5. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            rating_list.append({\n",
    "                'userId': new_user_id,\n",
    "                'movieId': int(row['movieId']),\n",
    "                'rating': rating\n",
    "            })\n",
    "        except:\n",
    "            print(\"Invalid rating. Skipping...\")\n",
    "\n",
    "    return rating_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're struggling to come up with the above function, you can use this list of user ratings to complete the next segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Movie: Revenge of the Nerds II: Nerds in Paradise (1987) | Genres: Comedy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Movie: Tropic Thunder (2008) | Genres: Action|Adventure|Comedy|War\n",
      "\n",
      "Movie: In July (Im Juli) (2000) | Genres: Comedy|Romance\n",
      "\n",
      "Movie: Amazon Women on the Moon (1987) | Genres: Comedy|Sci-Fi\n",
      "\n",
      "Movie: Journey 2: The Mysterious Island (2012) | Genres: Action|Adventure|Comedy|Sci-Fi|IMAX\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'userId': 193610, 'movieId': 27178, 'rating': 5.0},\n",
       " {'userId': 193610, 'movieId': 4079, 'rating': 5.0},\n",
       " {'userId': 193610, 'movieId': 92681, 'rating': 5.0}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ratings = movie_rater(df_movies, num=5, genre='Comedy')\n",
    "user_ratings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions With the New Ratings\n",
    "Now that you have new ratings, you can use them to make predictions for this new user. The proper way this should work is:\n",
    "\n",
    "* add the new ratings to the original ratings DataFrame, read into a `surprise` dataset \n",
    "* train a model using the new combined DataFrame\n",
    "* make predictions for the user\n",
    "* order those predictions from highest rated to lowest rated\n",
    "* return the top n recommendations with the text of the actual movie (rather than just the index number) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add the new ratings to the original ratings DataFrame\n",
    "# Convert list of dicts to DataFrame\n",
    "new_ratings_df = pd.DataFrame(user_ratings)\n",
    "\n",
    "# Append to original ratings DataFrame\n",
    "combined_ratings = pd.concat([new_df, new_ratings_df], ignore_index=True)\n",
    "\n",
    "combined_ratings.head()\n",
    "# Load combined ratings into Surprise dataset\n",
    "\n",
    "\n",
    "reader = Reader(rating_scale=(0.5, 5.0))\n",
    "\n",
    "data_combined = Dataset.load_from_df(\n",
    "    combined_ratings[['userId', 'movieId', 'rating']],\n",
    "    reader\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x2b5049570d0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train a model using the new combined DataFrame\n",
    "# Use the best parameters you found\n",
    "svd_new = SVD(n_factors=50, reg_all=0.05)\n",
    "\n",
    "# Train on full combined dataset\n",
    "trainset = data_combined.build_full_trainset()\n",
    "svd_new.fit(trainset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 4.27833989381878),\n",
       " (2, 3.868292434734969),\n",
       " (3, 3.6490136211503637),\n",
       " (4, 3.449692796862232),\n",
       " (5, 3.3606272839430704),\n",
       " (6, 4.356401983178466),\n",
       " (7, 3.4874386273464895),\n",
       " (8, 3.5772367949992705),\n",
       " (9, 3.3845526047583148),\n",
       " (10, 3.8071623092945805)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make predictions for the user\n",
    "# you'll probably want to create a list of tuples in the format (movie_id, predicted_score)\n",
    "new_user_id = new_ratings_df['userId'].iloc[0]\n",
    "\n",
    "# All movie IDs in the dataset\n",
    "all_movie_ids = df_movies['movieId'].unique()\n",
    "\n",
    "# Movies already rated by the user\n",
    "rated_movie_ids = new_ratings_df['movieId'].unique()\n",
    "\n",
    "# Filter out already rated movies\n",
    "unrated_movies = [m for m in all_movie_ids if m not in rated_movie_ids]\n",
    "predictions = []\n",
    "\n",
    "for movie_id in unrated_movies:\n",
    "    pred = svd_new.predict(new_user_id, movie_id)\n",
    "    predictions.append((movie_id, pred.est))\n",
    "predictions[:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1104, 4.762043357909304),\n",
       " (1204, 4.728169895600267),\n",
       " (318, 4.727411522111893),\n",
       " (7153, 4.69346321984254),\n",
       " (750, 4.655678304601412),\n",
       " (2959, 4.645714475904911),\n",
       " (1276, 4.6393233349211185),\n",
       " (58559, 4.635212748769364),\n",
       " (904, 4.631611297155196),\n",
       " (56782, 4.627023075547773)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# order the predictions from highest to lowest rated\n",
    "\n",
    "ranked_movies = sorted(predictions, key=lambda x: x[1], reverse=True)\n",
    "ranked_movies[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For the final component of this challenge, it could be useful to create a function `recommended_movies()` that takes in the parameters:\n",
    "* `user_ratings`: list - list of tuples formulated as (user_id, movie_id) (should be in order of best to worst for this individual)\n",
    "* `movie_title_df`: DataFrame \n",
    "* `n`: int - number of recommended movies \n",
    "\n",
    "The function should use a `for` loop to print out each recommended *n* movies in order from best to worst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the top n recommendations using the \n",
    "def recommended_movies(user_ratings,movie_title_df,n):\n",
    "        pass\n",
    "            \n",
    "recommended_movies(ranked_movies,df_movies,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level Up (Optional)\n",
    "\n",
    "* Try and chain all of the steps together into one function that asks users for ratings for a certain number of movies, then all of the above steps are performed to return the top $n$ recommendations\n",
    "* Make a recommender system that only returns items that come from a specified genre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you got the chance to implement a collaborative filtering model as well as retrieve recommendations from that model. You also got the opportunity to add your own recommendations to the system to get new recommendations for yourself! Next, you will learn how to use Spark to make recommender systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
